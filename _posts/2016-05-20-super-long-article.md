---
layout: post
title: "Miscellaneous on Programming Languages, and others"
categories: misc
---

CFG - does it stand for Control Flow Graph? Nope, just forget about computer science things at the moment... So, CFG actually represents major chords appearing in the C major according to the progression I, IV, V in basic chord theory of music. Similarly, does GCD mean greatest common divisor? No again ~.~, it is the chord progression I, IV, V for the G major. Learn the basic chord theory [here](http://www.start-playing-guitar.com/basic-chord-theory.html).

I tend to write stuff to keep it short in mind those things that I think are interesting. Here are some of those stuff that I sometimes revisit to remind myself.

**Curiosity-driven research:** “Millions saw the apple fall, but Newton asked why." - Bernard Baruch. I believe curiosity is always the primary factor that triggers creativity,and always keeping curiosity in mind is a great way of doing research. I have made some research papers out of my own curiority and eagerness. One of the works finally goes to a top conference (FSE17). Clearly, this is just a tiny sand in this enormous world, but it makes me very happy with how I developed myself in an independent and curiosity-driven thinking.

**Programming languages stuff:** 
Just some things that may be basic but I find interesting...
If you don't like to explore the beauty/application of logic in programming languages, whoops, just ignore this. Various kinds of logic have been floating around and successfully adapted to computer science. An example is constructive logic - the foundation of type systems. Here is a story that I found interesting when I learn the logic during the course of my PhD. "Proofs-as-Programs" - does the name sound interesting/meaningful to you? If it does not, well, I will personally interpret this as "constructing programs based on logic proofs". Still vague? Let's see an example.
Given a specification of a function in functional programming language, SML for example, which specifies the input and output types of the function, we can synthesize an implementation of the function that satisfies the specification. At this point, I was surprised when trying to think about this...Hey, how do we pull the function implementation out of thin air like this? It turned out that this is possible, but how? By using Curry-Howard isomorphism, the specification can be converted to a corresponding proposition that we need to prove. Let's take a piece of paper, write down the constructive proof for it, and be prepared that that piece of paper is way too small for the proof (lol). We can then construct proof term for the proof and then convert the proof term back to an actual implementation of the function.
Learn more about this logic stuff [here](https://books.google.com/books?id=ClRuzQj1rPIC&pg=PR7&lpg=PR7&dq=curry+howard+isomorphism+in+program+synthesis&source=bl&ots=0JxOwa4DDJ&sig=ZoFge9n9ZlAeuyp7ozejR2ESVnU&hl=en&sa=X&ved=0CDkQ6AEwBWoVChMIsoXLy6CvyAIVjDY-Ch0ZbgO_#v=onepage&q=curry%20howard%20isomorphism%20in%20program%20synthesis&f=false).
 
**Sequent Calculus:** I implemented a proof search procedure for G4ip sequent calculus using SML [here](https://github.com/xuanbachle/G4ip). (Note there is a known bug in my code, if you are interested please fix it :) ). The idea of the G4ip is to take into account the invertibility of proof rules. That is, a rule is invertible if the conclusion holds iff the premises hold. During a backward proof search procedure, proof search is going bottom up. The invertible rules help us search for proof by going up without the need of backtracking while not losing any information (remember the characteristics of invertible rules). We therefore favor invertible rules in proof search procedure until all the invertible rules are exhausted. If all invertible rules have been used but proof has not been found yet, we then apply non-invertible rules for the proof search.
I found the reduced sequent calculus interesting because it improves natural deduction and regular sequent calculus in both context (assumptions) management and efficiency in proof search. Learn a bit more about sequent calculus here and soon I will find some materials on reduced sequent calculus and natural deduction to take note here.
 
I liked solving math stuff back when I was in high school. I was lucky to be accepted to a mathematics-specialized class for gifted students. During high school time, I find it interesting when solving algebra, geometry, etc. I studied hard and got the first prize in provincial mathematics Olympiads, and subsequently got selected for top 10 potential delegates to attend the national mathematics Olympiads. Till now, although my research mainly focuses on software engineering, I still enjoy reading papers in programming languages area such as logic, verification, etc with beautiful applications of mathematics. In fact, I did have a chance to work on this area during my time at NUS with a very very nice advisor there, but well, I guess following that path is not my destiny :D.
 
**Cut and Natural Deduction:** Just put here to remind myself writing something about this later. The idea is that Cut and Natural Deduction are both great tool for humans to do proof but not so great for machines to search for proof. The main reason is that the machine would need to guess something out from nowhere to do Cut proof, while natural deduction may suffer from infinite loop because it has no explicit context management which may cause the proof search procedure to repeat itself the same proof that it needs to prove over and over again, whoops, proof search spins around~~

**Counterexample-guided Synthesis:** Synthesis problems supplied with logical specifications are often expensive without heuristics to reduce the number of queries to SMT solvers. Counterexample-guided synthesis is one way to do so. Particularly, instead of querying an SMT solver for correctness validation every time a candidate solution is found, we iteratively find a set of concrete examples that under-approximates the provided logical specifications. If the candidate fails to satisfy the concrete examples (through dynamically testing the candidate against the examples), we can discard the candidate right way. Otherwise, the candidate is checked against the logical specifications via querying a solver. If the candidate fails to meet the specifications, we discard it, while generating a counterexample that witnesses the failure. This process is repeated until a candidate that satisfies the specifications is found. This sounds simple, but then a natural technical question is, how the counterexample can be iteratively generated? Hmm, interesting. This is where a trick in checking "satisfiability" comes into play. Suppose we want to check if a candidate solution A satisfies the specification S, we check if substituting A into S would make the formula (Not (S)) satisfiable. If Not(S) is satisfiable, it means that S is not satisfiable, which implies that A does not satisfy the specification. In this case, we can just extract an example (assignment to variables appearing in A) that makes Not(S) satisfiable, witnessing the failure of A. The real trick here is that when a formula is satisfiable, the underlying solver will return an assignment to variables in the formula as a solution.

**Java Memory Model:**  A great blog post on Java memory model [here](http://tutorials.jenkov.com/java-concurrency/java-memory-model.html). It is extremely important to understand the Java memory model to write concurrency code in Java.

**Generating Function:** An application of generating function is to the counting problem. I am amazed when reading an example of generating function for generalizing Fibonacci series. Take a look [here](https://web.williams.edu/Mathematics/sjmiller/public_html/331/handouts/ProbLifesaver_GeneratingFns.pdf) and the beautiful Taylor series [here](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_chap12.pdf)

A good explanation on [Diffie-Hellman](https://www.math.cornell.edu/~mec/2003-2004/cryptography/diffiehellman/diffiehellman.html) key exchange. An interesting book for [mathematics for computer science.](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_notes.pdf)

Japanese stuff:

日本語は面白い！

大学で、ずっと日本語を勉強していましたけど、最近使うチャンスがほとんどないからなんか忘れちゃったんだ。日本語を勉強することが始まったのは２００７ からです。その時は、大学に入ったばかりだから、何でも新しい。しかし、私に日本語を勉強するのはなんか何より特別感があると思う。日本語を勉強すればす るほど、面白くなってきた。
